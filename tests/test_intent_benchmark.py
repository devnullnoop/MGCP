"""Intent Routing Benchmark: Regex vs Graph-Community vs LLM Self-Routing.

Compares three approaches for classifying user messages into actionable intents:

1. **Regex** (baseline) — Extracts patterns from MGCP's hook scripts (init_project.py).
   Replicates the exact behavior of git-reminder, catalogue-reminder, and task-start hooks.

2. **Graph-Community** — Seeds bootstrap lessons into a clean Qdrant + NetworkX instance,
   runs Louvain community detection, generates summaries, then classifies by embedding
   the user message and searching community summaries.

3. **LLM Self-Routing** — A compact routing prompt is given to the LLM along with each
   user message. The LLM classifies intent using language understanding. Results are
   stored as a snapshot (llm_classifications.yaml) generated by Claude Opus 4.6.

All classifiers are graded against the same ground-truth corpus (intent_corpus.yaml).

Usage:
    pytest tests/test_intent_benchmark.py -v -s
    pytest tests/test_intent_benchmark.py -v -s -k benchmark_report
"""

import re
import tempfile
from collections import defaultdict
from pathlib import Path

import pytest
import yaml

from mgcp.bootstrap_loader import load_lessons, load_relationships
from mgcp.graph import LessonGraph
from mgcp.persistence import LessonStore
from mgcp.qdrant_vector_store import QdrantVectorStore

pytestmark = pytest.mark.slow

# ---------------------------------------------------------------------------
# Corpus + snapshot loaders
# ---------------------------------------------------------------------------

BENCHMARK_DIR = Path(__file__).parent / "benchmark_data"
CORPUS_PATH = BENCHMARK_DIR / "intent_corpus.yaml"
LLM_SNAPSHOT_PATH = BENCHMARK_DIR / "llm_classifications.yaml"
LLM_BLIND_PATH = BENCHMARK_DIR / "llm_blind_classifications.yaml"

# The routing prompt that would be injected by the single hook.
# This is the exact text the LLM sees before classifying.
ROUTING_PROMPT = """\
Classify the user's message into zero or more intents. Only include intents
where the user is clearly performing or requesting that action — not merely
mentioning the topic in passing.

Intents:
- git_operation: User wants to commit, push, merge, deploy, create a PR, or ship code
- catalogue_dependency: User mentions adopting, installing, or choosing a library/package/framework
- catalogue_security: User identifies a security vulnerability, auth weakness, or exploit risk
- catalogue_decision: User announces a technical choice ("went with X over Y", "decided on X")
- catalogue_arch_note: User flags a gotcha, quirk, caveat, or surprising behavior
- catalogue_convention: User states a coding rule, naming convention, or style standard
- task_start: User wants to fix, implement, build, refactor, debug, or set up something

If none apply, return: none"""


def load_corpus() -> list[dict]:
    """Load the ground-truth intent corpus."""
    with open(CORPUS_PATH) as f:
        data = yaml.safe_load(f)
    return data["messages"]


def _load_snapshot(path: Path) -> dict[str, set[str]]:
    """Load a classification snapshot as {text: set(intents)}."""
    with open(path) as f:
        data = yaml.safe_load(f)

    result = {}
    for entry in data["classifications"]:
        result[entry["text"]] = set(entry["intents"])
    return result


def load_llm_snapshot() -> dict[str, set[str]]:
    """Load contaminated LLM snapshot (Opus, saw corpus)."""
    return _load_snapshot(LLM_SNAPSHOT_PATH)


def load_llm_blind() -> dict[str, set[str]]:
    """Load blind LLM snapshot (Haiku, clean context per message)."""
    return _load_snapshot(LLM_BLIND_PATH)


def classify_llm(text: str, snapshot: dict[str, set[str]]) -> set[str]:
    """Look up LLM classification from snapshot."""
    return snapshot.get(text, {"none"})


# ---------------------------------------------------------------------------
# Classifier 1: Regex (replicates hook behavior exactly)
# ---------------------------------------------------------------------------

# Patterns lifted verbatim from init_project.py hook scripts
GIT_KEYWORDS = [
    r"\bcommit\b", r"\bpush\b", r"\bgit\b", r"\bpr\b",
    r"\bpull request\b", r"\bmerge\b",
]

CATALOGUE_PATTERNS = [
    (r"\b(using|chose|picked|selected|installed|added)\b.{0,30}\b(library|package|framework|tool)\b", "catalogue_dependency"),
    (r"\b(pip install|npm install|cargo add|go get)\b", "catalogue_dependency"),
    (r"\b(security|vulnerability|cve|exploit)\b.{0,20}\b(issue|bug|concern|risk)\b", "catalogue_security"),
    (r"\b(decided|choosing|picked|went with)\b.{0,20}\b(over|instead of)\b", "catalogue_decision"),
    (r"\b(gotcha|quirk|caveat|watch out|careful|tricky)\b", "catalogue_arch_note"),
    (r"\b(convention|naming|style|always|never)\b.{0,20}\b(use|follow|do|avoid)\b", "catalogue_convention"),
]

TASK_START_PATTERNS = [
    r"\b(fix|fixing|debug|debugging)\b.{0,30}\b(bug|issue|error|problem)\b",
    r"\b(implement|implementing|add|adding|create|creating|build|building)\b.{0,30}\b(feature|function|method|class|component|endpoint|api)\b",
    r"\b(refactor|refactoring|restructure|reorganize|clean up)\b",
    r"\b(update|updating|change|changing|modify|modifying)\b.{0,20}\b(the|this|that)\b",
    r"\blet'?s\s+(fix|implement|add|create|build|refactor|update|change|work on)\b",
    r"\b(can you|could you|please|i need you to)\b.{0,20}\b(fix|implement|add|create|build|refactor|update)\b",
    r"\b(work on|working on|tackle|tackling|handle|handling)\b",
    r"\b(set up|setting up|configure|configuring|install|installing)\b",
]

TASK_IGNORE_PATTERNS = [
    r"\b(commit|push|pull|merge|git)\b",
    r"\b(library|package|framework|security|convention)\b",
]


def classify_regex(text: str) -> set[str]:
    """Classify a user message using the regex hook patterns.

    Replicates the precedence logic from the hooks:
    - git-reminder fires first and exits
    - catalogue-reminder fires next
    - task-start fires last, but IGNORES messages matching git/catalogue keywords
    """
    intents: set[str] = set()
    lower = text.lower()

    # 1. Git check (git-reminder hook)
    for pattern in GIT_KEYWORDS:
        if re.search(pattern, lower, re.IGNORECASE):
            intents.add("git_operation")
            break

    # 2. Catalogue check (catalogue-reminder hook)
    for pattern, intent in CATALOGUE_PATTERNS:
        if re.search(pattern, lower, re.IGNORECASE):
            intents.add(intent)

    # 3. Task-start check (task-start hook, with deconfliction)
    ignored = False
    for pattern in TASK_IGNORE_PATTERNS:
        if re.search(pattern, lower, re.IGNORECASE):
            ignored = True
            break

    if not ignored:
        for pattern in TASK_START_PATTERNS:
            if re.search(pattern, lower, re.IGNORECASE):
                intents.add("task_start")
                break

    return intents if intents else {"none"}


# ---------------------------------------------------------------------------
# Classifier 2: Graph-Community
# ---------------------------------------------------------------------------

TAG_TO_INTENT = {
    "git": "git_operation",
    "version-control": "git_operation",
    "branching": "git_operation",
    "commits": "git_operation",
    "dependencies": "catalogue_dependency",
    "supply-chain": "catalogue_dependency",
    "package-management": "catalogue_dependency",
    "security": "catalogue_security",
    "owasp": "catalogue_security",
    "authentication": "catalogue_security",
    "authorization": "catalogue_security",
    "encryption": "catalogue_security",
    "input-validation": "catalogue_security",
    "xss": "catalogue_security",
    "sql-injection": "catalogue_security",
    "csrf": "catalogue_security",
    "session-management": "catalogue_security",
    "crypto": "catalogue_security",
    "data-protection": "catalogue_security",
    "architecture": "catalogue_arch_note",
    "gotcha": "catalogue_arch_note",
    "performance": "catalogue_arch_note",
    "caching": "catalogue_arch_note",
    "error-handling": "catalogue_arch_note",
    "naming": "catalogue_convention",
    "style": "catalogue_convention",
    "code-quality": "catalogue_convention",
    "linting": "catalogue_convention",
    "debugging": "task_start",
    "testing": "task_start",
    "implementation": "task_start",
    "refactoring": "task_start",
    "development": "task_start",
    "feature": "task_start",
    "bug": "task_start",
    "fix": "task_start",
}


def generate_community_summary(community: dict, graph: LessonGraph) -> str:
    """Auto-generate a searchable summary from community member data."""
    parts = []
    for member_id in community["members"]:
        node = graph.graph.nodes.get(member_id, {})
        trigger = node.get("trigger", "")
        action = node.get("action", "")
        tags = node.get("tags", [])
        if trigger:
            parts.append(trigger)
        if action:
            parts.append(action)
        if tags:
            parts.append(" ".join(tags))

    agg_tags = community.get("aggregate_tags", {})
    if agg_tags:
        parts.append("Keywords: " + ", ".join(agg_tags.keys()))

    return " | ".join(parts[:50])


def community_tags_to_intents(community: dict) -> set[str]:
    """Map a community's aggregate tags to intent labels."""
    intents = set()
    for tag in community.get("aggregate_tags", {}):
        tag_lower = tag.lower()
        if tag_lower in TAG_TO_INTENT:
            intents.add(TAG_TO_INTENT[tag_lower])
    return intents


class GraphCommunityClassifier:
    """Classifies user messages using graph community structure + Qdrant search."""

    def __init__(self, vector_store: QdrantVectorStore, graph: LessonGraph, threshold: float = 0.40):
        self.vector_store = vector_store
        self.graph = graph
        self.threshold = threshold
        self.communities = graph.detect_communities(resolution=1.0)
        self.community_intents: dict[str, set[str]] = {}

        for comm in self.communities:
            self.community_intents[comm["community_id"]] = community_tags_to_intents(comm)

        for comm in self.communities:
            summary_text = generate_community_summary(comm, graph)
            self.vector_store.upsert_community_summary(
                community_id=comm["community_id"],
                searchable_text=summary_text,
                metadata={
                    "title": f"Community {comm['community_id'][:8]}",
                    "member_count": comm["size"],
                    "tags": ",".join(comm.get("aggregate_tags", {}).keys()),
                },
            )

    def classify(self, text: str) -> set[str]:
        """Classify a user message by searching community summaries."""
        intents: set[str] = set()

        results = self.vector_store.query_community_summaries(text, limit=5)
        for community_id, score, metadata in results:
            if score >= self.threshold:
                comm_intents = self.community_intents.get(community_id, set())
                intents.update(comm_intents)

        lesson_results = self.vector_store.search(text, limit=5, min_score=self.threshold)
        for lesson_id, score in lesson_results:
            node = self.graph.graph.nodes.get(lesson_id, {})
            for tag in node.get("tags", []):
                tag_lower = tag.lower()
                if tag_lower in TAG_TO_INTENT:
                    intents.add(TAG_TO_INTENT[tag_lower])

        return intents if intents else {"none"}


# ---------------------------------------------------------------------------
# Grading metrics
# ---------------------------------------------------------------------------

def compute_per_intent_metrics(
    predictions: list[set[str]], ground_truth: list[set[str]], intent_labels: list[str],
) -> dict[str, dict[str, float]]:
    """Compute precision, recall, F1 per intent label using multi-label scoring."""
    metrics = {}
    for label in intent_labels:
        tp = fp = fn = 0
        for pred, truth in zip(predictions, ground_truth):
            if label in pred and label in truth:
                tp += 1
            elif label in pred and label not in truth:
                fp += 1
            elif label not in pred and label in truth:
                fn += 1

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        support = tp + fn

        metrics[label] = {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "support": support,
            "tp": tp,
            "fp": fp,
            "fn": fn,
        }

    return metrics


def compute_aggregate_metrics(
    predictions: list[set[str]], ground_truth: list[set[str]],
) -> dict[str, float]:
    """Compute aggregate metrics across all messages."""
    exact_match = sum(1 for p, t in zip(predictions, ground_truth) if p == t)
    total = len(predictions)

    coverage = sum(
        1 for p, t in zip(predictions, ground_truth) if t.issubset(p)
    )

    none_messages = [(p, t) for p, t in zip(predictions, ground_truth) if t == {"none"}]
    none_fp = sum(1 for p, _ in none_messages if p != {"none"})

    return {
        "exact_match": exact_match / total if total else 0.0,
        "coverage": coverage / total if total else 0.0,
        "none_fp_rate": none_fp / len(none_messages) if none_messages else 0.0,
        "total": total,
    }


# ---------------------------------------------------------------------------
# Fixtures
# ---------------------------------------------------------------------------

@pytest.fixture(scope="module")
def corpus():
    """Load the ground-truth corpus."""
    return load_corpus()


@pytest.fixture(scope="module")
def llm_snapshot():
    """Load the contaminated LLM classification snapshot (Opus, saw corpus)."""
    return load_llm_snapshot()


@pytest.fixture(scope="module")
def llm_blind():
    """Load the blind LLM classification snapshot (Haiku, clean context)."""
    return load_llm_blind()


@pytest.fixture(scope="module")
def graph_classifier():
    """Bootstrap a clean MGCP instance and build the graph-community classifier.

    Module-scoped to avoid re-seeding on every test (~30s setup).
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = str(Path(tmpdir) / "test.db")
        qdrant_path = str(Path(tmpdir) / "qdrant")

        store = LessonStore(db_path=db_path)
        vector_store = QdrantVectorStore(persist_path=qdrant_path)
        graph = LessonGraph()

        import asyncio

        async def seed():
            from mgcp.bootstrap import seed_lessons, seed_relationships

            core_lessons = load_lessons("core")
            dev_lessons = load_lessons("dev")
            all_lessons = core_lessons + dev_lessons

            core_rels = load_relationships("core")
            dev_rels = load_relationships("dev")
            all_rels = core_rels + dev_rels

            await seed_lessons(all_lessons, store, vector_store, graph)
            await seed_relationships(all_rels, store, graph)

            return len(all_lessons), len(all_rels)

        n_lessons, n_rels = asyncio.run(seed())
        print(f"\n[Setup] Seeded {n_lessons} lessons, {n_rels} relationships")
        print(f"[Setup] Graph: {graph.graph.number_of_nodes()} nodes, {graph.graph.number_of_edges()} edges")

        classifier = GraphCommunityClassifier(vector_store, graph, threshold=0.40)
        print(f"[Setup] Detected {len(classifier.communities)} communities")
        for comm in classifier.communities[:5]:
            tags = list(comm.get("aggregate_tags", {}).keys())[:5]
            intents = community_tags_to_intents(comm)
            print(f"  Community {comm['community_id'][:8]}: {comm['size']} members, "
                  f"tags={tags}, intents={intents}")

        yield classifier


# ---------------------------------------------------------------------------
# Tests
# ---------------------------------------------------------------------------

ALL_INTENT_LABELS = [
    "git_operation", "catalogue_dependency", "catalogue_security",
    "catalogue_decision", "catalogue_arch_note", "catalogue_convention",
    "task_start", "none",
]


class TestRegexClassifier:
    """Verify the regex classifier matches current hook behavior."""

    def test_direct_git(self):
        assert "git_operation" in classify_regex("let's commit this")

    def test_direct_catalogue_dependency(self):
        assert "catalogue_dependency" in classify_regex("I installed the requests library for HTTP calls")

    def test_direct_catalogue_security(self):
        # Regex requires (security|vulnerability|cve|exploit) + (issue|bug|concern|risk)
        assert "catalogue_security" in classify_regex("there's a security vulnerability issue in auth")

    def test_direct_catalogue_decision(self):
        assert "catalogue_decision" in classify_regex("decided to use PostgreSQL over SQLite for the backend")

    def test_direct_catalogue_arch_note(self):
        assert "catalogue_arch_note" in classify_regex("there's a gotcha with the connection pool timeout")

    def test_direct_catalogue_convention(self):
        assert "catalogue_convention" in classify_regex("we always use snake_case for function names")

    def test_direct_task_start(self):
        assert "task_start" in classify_regex("implement the export feature")

    def test_task_start_deconfliction(self):
        """Task-start hook should NOT fire when git/catalogue keywords are present."""
        result = classify_regex("fix the security vulnerability in auth")
        assert "task_start" not in result

    def test_no_intent(self):
        assert classify_regex("thanks") == {"none"}

    def test_false_positive_git_book(self):
        """'git book' contains 'git' keyword — regex WILL false-positive here."""
        result = classify_regex("the git book was really interesting")
        assert "git_operation" in result


class TestGraphClassifier:
    """Verify the graph-community classifier produces reasonable results."""

    def test_classifier_initialized(self, graph_classifier):
        assert len(graph_classifier.communities) > 0

    def test_classify_returns_set(self, graph_classifier):
        result = graph_classifier.classify("let's commit this")
        assert isinstance(result, set)

    def test_neutral_message(self, graph_classifier):
        result = graph_classifier.classify("ok")
        assert isinstance(result, set)


class TestLLMClassifier:
    """Verify both LLM snapshots cover the full corpus."""

    def test_snapshot_covers_corpus(self, corpus, llm_snapshot):
        """Every corpus message should have a contaminated LLM classification."""
        missing = [m["text"] for m in corpus if m["text"] not in llm_snapshot]
        assert not missing, f"LLM snapshot missing {len(missing)} messages: {missing[:3]}"

    def test_blind_covers_corpus(self, corpus, llm_blind):
        """Every corpus message should have a blind LLM classification."""
        missing = [m["text"] for m in corpus if m["text"] not in llm_blind]
        assert not missing, f"LLM blind missing {len(missing)} messages: {missing[:3]}"

    def test_snapshot_returns_valid_intents(self, llm_snapshot):
        """All contaminated LLM classifications should use valid intent labels."""
        valid = set(ALL_INTENT_LABELS)
        for text, intents in llm_snapshot.items():
            invalid = intents - valid
            assert not invalid, f"Invalid intents {invalid} for '{text}'"

    def test_blind_returns_valid_intents(self, llm_blind):
        """All blind LLM classifications should use valid intent labels."""
        valid = set(ALL_INTENT_LABELS)
        for text, intents in llm_blind.items():
            invalid = intents - valid
            assert not invalid, f"Invalid intents {invalid} for '{text}'"


class TestBenchmarkReport:
    """Run the full 3-way benchmark comparison and print the report."""

    def test_benchmark_report(self, corpus, graph_classifier, llm_blind):
        """Run all three classifiers on the full corpus and print comparison.

        Uses the BLIND LLM results (Haiku, clean context) as the primary LLM column.
        """
        messages = corpus
        texts = [m["text"] for m in messages]
        truths = [set(m["intents"]) for m in messages]
        categories = [m["category"] for m in messages]

        # Run classifiers
        regex_preds = [classify_regex(t) for t in texts]
        graph_preds = [graph_classifier.classify(t) for t in texts]
        llm_preds = [classify_llm(t, llm_blind) for t in texts]

        # Compute metrics
        regex_pi = compute_per_intent_metrics(regex_preds, truths, ALL_INTENT_LABELS)
        graph_pi = compute_per_intent_metrics(graph_preds, truths, ALL_INTENT_LABELS)
        llm_pi = compute_per_intent_metrics(llm_preds, truths, ALL_INTENT_LABELS)
        regex_agg = compute_aggregate_metrics(regex_preds, truths)
        graph_agg = compute_aggregate_metrics(graph_preds, truths)
        llm_agg = compute_aggregate_metrics(llm_preds, truths)

        regex_mf1 = sum(m["f1"] for m in regex_pi.values()) / len(regex_pi)
        graph_mf1 = sum(m["f1"] for m in graph_pi.values()) / len(graph_pi)
        llm_mf1 = sum(m["f1"] for m in llm_pi.values()) / len(llm_pi)

        # Print report
        print("\n")
        print("=" * 100)
        print("INTENT ROUTING BENCHMARK: Regex vs Graph-Community vs LLM Self-Routing (Haiku, blind)")
        print("=" * 100)

        # Per-intent table
        print(f"\n{'':25} | {'--- REGEX ---':^17} | {'--- GRAPH ---':^17} | {'- LLM BLIND -':^17} |")
        print(f"{'Intent':<25} | {'P':>4} {'R':>4} {'F1':>5} {'#':>2} "
              f"| {'P':>4} {'R':>4} {'F1':>5} {'#':>2} "
              f"| {'P':>4} {'R':>4} {'F1':>5} {'#':>2} | Winner")
        print("-" * 100)

        for label in ALL_INTENT_LABELS:
            rm, gm, lm = regex_pi[label], graph_pi[label], llm_pi[label]

            scores = {"REGEX": rm["f1"], "GRAPH": gm["f1"], "LLM": lm["f1"]}
            best_score = max(scores.values())
            if best_score == 0 and rm["support"] == 0:
                winner = ""
            else:
                winners = [k for k, v in scores.items() if v >= best_score - 0.05]
                winner = winners[0] if len(winners) == 1 else "TIE" if len(winners) > 1 else ""

            print(
                f"{label:<25} "
                f"| {rm['precision']:4.2f} {rm['recall']:4.2f} {rm['f1']:5.2f} {rm['support']:2d} "
                f"| {gm['precision']:4.2f} {gm['recall']:4.2f} {gm['f1']:5.2f} {gm['support']:2d} "
                f"| {lm['precision']:4.2f} {lm['recall']:4.2f} {lm['f1']:5.2f} {lm['support']:2d} "
                f"| {winner}"
            )

        # Aggregate
        print(f"\n{'AGGREGATE METRICS':^100}")
        print("-" * 100)
        print(f"  {'Metric':<25} | {'Regex':>10} | {'Graph':>10} | {'LLM':>10}")
        print(f"  {'-'*25}-+-{'-'*10}-+-{'-'*10}-+-{'-'*10}")
        print(f"  {'Exact Match':<25} | {regex_agg['exact_match']:>9.0%} | {graph_agg['exact_match']:>9.0%} | {llm_agg['exact_match']:>9.0%}")
        print(f"  {'Coverage':<25} | {regex_agg['coverage']:>9.0%} | {graph_agg['coverage']:>9.0%} | {llm_agg['coverage']:>9.0%}")
        print(f"  {'Macro F1':<25} | {regex_mf1:>9.2f} | {graph_mf1:>9.2f} | {llm_mf1:>9.2f}")
        print(f"  {'None FP Rate':<25} | {regex_agg['none_fp_rate']:>9.0%} | {graph_agg['none_fp_rate']:>9.0%} | {llm_agg['none_fp_rate']:>9.0%}")
        print(f"  {'Total Messages':<25} | {regex_agg['total']:>10d} | {graph_agg['total']:>10d} | {llm_agg['total']:>10d}")

        # Routing prompt for reference
        print(f"\n{'LLM ROUTING PROMPT':^100}")
        print("-" * 100)
        for line in ROUTING_PROMPT.split("\n"):
            print(f"  {line}")

        # Detailed mismatches — only show where at least one classifier got it wrong
        print(f"\n{'DETAILED RESULTS':^100}")
        print("-" * 100)

        by_category: dict[str, list] = defaultdict(list)
        for i, msg in enumerate(messages):
            truth = truths[i]
            r_pred, g_pred, l_pred = regex_preds[i], graph_preds[i], llm_preds[i]
            r_ok = r_pred == truth
            g_ok = g_pred == truth
            l_ok = l_pred == truth

            by_category[categories[i]].append({
                "text": texts[i],
                "truth": truth,
                "regex": r_pred, "graph": g_pred, "llm": l_pred,
                "r_ok": r_ok, "g_ok": g_ok, "l_ok": l_ok,
            })

        for category in ["direct", "indirect", "false_positive", "multi_intent", "no_intent", "edge_case"]:
            entries = by_category.get(category, [])
            if not entries:
                continue

            n_ok_r = sum(1 for e in entries if e["r_ok"])
            n_ok_g = sum(1 for e in entries if e["g_ok"])
            n_ok_l = sum(1 for e in entries if e["l_ok"])
            total = len(entries)
            print(f"\n  [{category.upper()}] Regex: {n_ok_r}/{total}  Graph: {n_ok_g}/{total}  LLM: {n_ok_l}/{total}")

            for m in entries:
                r_s = "OK" if m["r_ok"] else _fmt_set(m["regex"])
                g_s = "OK" if m["g_ok"] else _fmt_set(m["graph"])
                l_s = "OK" if m["l_ok"] else _fmt_set(m["llm"])
                # Mark with symbols for quick scanning
                indicators = (
                    ("R" if m["r_ok"] else ".") +
                    ("G" if m["g_ok"] else ".") +
                    ("L" if m["l_ok"] else ".")
                )
                print(f"    [{indicators}] \"{m['text']}\"")
                if not (m["r_ok"] and m["g_ok"] and m["l_ok"]):
                    print(f"           truth={_fmt_set(m['truth'])}  "
                          f"R={r_s}  G={g_s}  L={l_s}")

        # Summary
        regex_n = sum(1 for p, t in zip(regex_preds, truths) if p == t)
        graph_n = sum(1 for p, t in zip(graph_preds, truths) if p == t)
        llm_n = sum(1 for p, t in zip(llm_preds, truths) if p == t)
        print(f"\n{'=' * 100}")
        print(f"SUMMARY: Regex {regex_n}/{len(messages)}  |  Graph {graph_n}/{len(messages)}  |  LLM {llm_n}/{len(messages)}")
        print(f"{'=' * 100}")

    def test_category_breakdown(self, corpus, graph_classifier, llm_blind):
        """Show performance broken down by corpus category."""
        messages = corpus
        categories = ["direct", "indirect", "false_positive", "multi_intent", "no_intent", "edge_case"]

        print(f"\n{'PERFORMANCE BY CATEGORY':^100}")
        print("=" * 100)
        print(f"  {'Category':<18} | {'Regex':>8} | {'Graph':>8} | {'LLM':>8} | {'Winner':>8}")
        print(f"  {'-'*18}-+-{'-'*8}-+-{'-'*8}-+-{'-'*8}-+-{'-'*8}")

        for cat in categories:
            cat_msgs = [m for m in messages if m["category"] == cat]
            if not cat_msgs:
                continue

            r_n = sum(1 for m in cat_msgs if classify_regex(m["text"]) == set(m["intents"]))
            g_n = sum(1 for m in cat_msgs if graph_classifier.classify(m["text"]) == set(m["intents"]))
            l_n = sum(1 for m in cat_msgs if classify_llm(m["text"], llm_blind) == set(m["intents"]))
            total = len(cat_msgs)

            scores = {"REGEX": r_n, "GRAPH": g_n, "LLM": l_n}
            best = max(scores.values())
            winners = [k for k, v in scores.items() if v == best]
            winner = winners[0] if len(winners) == 1 else "TIE"

            print(
                f"  {cat:<18} | {r_n:>3}/{total:<3} | {g_n:>3}/{total:<3} | {l_n:>3}/{total:<3} | {winner:>8}"
            )


def _fmt_set(s: set) -> str:
    """Format a set of intents for display."""
    if not s:
        return "{}"
    return "{" + ", ".join(sorted(s)) + "}"
